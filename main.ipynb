{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0    We extend to natural deduction the approach ...    LO\n",
      "1    Over the last decade, the IEEE 802.11 has em...    NI\n",
      "2    Motivated by the problem of storing coloured...    DS\n",
      "3    We consider the downlink of a cellular syste...    NI\n",
      "4    Meroitic is the still undeciphered language ...    CL\n",
      "                                                text label\n",
      "0    Manne et al. designed the first algorithm co...    DC\n",
      "1    We consider the challenge of creating guidel...    SE\n",
      "2    Network virtualization techniques allow for ...    NI\n",
      "3    In the Min $k$-Cut problem, input is an edge...    DS\n",
      "4    We introduce the notion of being Weihrauch-c...    LO\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the datasets\n",
    "train_df = pd.read_csv('/home/administrator/personal/data/train.csv', header=None)\n",
    "validation_df = pd.read_csv('/home/administrator/personal/data/validation.csv', header=None)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "train_df.columns = ['text', 'label']\n",
    "validation_df.columns = ['text', 'label']\n",
    "\n",
    "# Display the first few rows to verify the structure\n",
    "print(train_df.head())\n",
    "print(validation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  extend natural deduction approach linear neste...    LO\n",
      "1  last decade ieee emerged popular protocol wire...    NI\n",
      "2  motivated problem storing coloured de bruijn g...    DS\n",
      "3  consider downlink cellular system address prob...    NI\n",
      "4  meroitic still undeciphered language ancient c...    CL\n",
      "                                                text label\n",
      "0  manne et al designed first algorithm computing...    DC\n",
      "1  consider challenge creating guideline evaluate...    SE\n",
      "2  network virtualization technique allow coexist...    NI\n",
      "3  min kcut problem input edge weighted graph g i...    DS\n",
      "4  introduce notion weihrauchcomplete layerwise c...    LO\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')  # Tokenizer models\n",
    "nltk.download('stopwords')  # Stopwords list\n",
    "nltk.download('wordnet')  # Lemmatizer models\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and non-alphabetic characters\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    stop_words = set(stopwords.words('english'))  # Define the set of English stopwords\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords from the tokenized words\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialize the lemmatizer\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize the words\n",
    "    return ' '.join(words)  # Join the processed words back into a single string\n",
    "\n",
    "# Apply preprocessing to the 'text' column of the training and validation DataFrames\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "validation_df['text'] = validation_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Verify the preprocessing by printing the first few rows of the DataFrames\n",
    "print(train_df.head())\n",
    "print(validation_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/miniconda3/envs/gaurav/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Initialize the BERT tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=7)  # Load BERT model for sequence classification with 7 labels\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()  # Initialize the label encoder\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df['label'])  # Fit label encoder on training labels and transform them to integers\n",
    "validation_labels_encoded = label_encoder.transform(validation_df['label'])  # Transform validation labels using the fitted encoder\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts  # List of texts\n",
    "        self.labels = labels  # Corresponding labels\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer\n",
    "        self.max_len = max_len  # Maximum sequence length for BERT input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Return the number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # Get the text at index `idx`\n",
    "        label = self.labels[idx]  # Get the corresponding label\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add special tokens ([CLS], [SEP]) to the text\n",
    "            max_length=self.max_len,  # Specify the maximum length of the sequence\n",
    "            return_token_type_ids=False,  # Do not return token type IDs\n",
    "            padding='max_length',  # Pad sequences to the maximum length\n",
    "            truncation=True,  # Truncate sequences longer than the maximum length\n",
    "            return_attention_mask=True,  # Return the attention mask to distinguish padded elements\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),  # Flatten input_ids tensor\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),  # Flatten attention_mask tensor\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor of type long\n",
    "        }\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataset = CustomDataset(train_df['text'].tolist(), train_labels_encoded, tokenizer, max_len=128)  # Initialize training dataset with texts, labels, and tokenizer\n",
    "validation_dataset = CustomDataset(validation_df['text'].tolist(), validation_labels_encoded, tokenizer, max_len=128)  # Initialize validation dataset with texts, labels, and tokenizer\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create DataLoader for training data, with batch size of 16 and shuffling enabled\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16)  # Create DataLoader for validation data, with batch size of 16 (no shuffling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4883, Accuracy: 0.8489\n",
      "Epoch 2, Loss: 0.2260, Accuracy: 0.9289\n",
      "Epoch 3, Loss: 0.1305, Accuracy: 0.9627\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW  # Import AdamW optimizer from PyTorch\n",
    "from transformers import get_linear_schedule_with_warmup  # Import learning rate scheduler from Hugging Face's Transformers\n",
    "from sklearn.metrics import accuracy_score  # Import accuracy_score from scikit-learn for calculating accuracy\n",
    "\n",
    "# Set up training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to GPU if available, otherwise use CPU\n",
    "model = model.to(device)  # Move the model to the chosen device (GPU or CPU)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # Initialize the AdamW optimizer with the model parameters and a learning rate of 2e-5\n",
    "total_steps = len(train_loader) * 3  # Calculate the total number of training steps (number of batches per epoch * number of epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)  # Set up a linear learning rate scheduler with warmup\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Loop over the number of epochs (3 in this case)\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    total_loss = 0  # Initialize the total loss for the epoch\n",
    "    correct_predictions = 0  # Initialize the count of correct predictions\n",
    "    total_samples = 0  # Initialize the count of total samples\n",
    "    \n",
    "    for batch in train_loader:  # Loop over each batch in the training DataLoader\n",
    "        input_ids = batch['input_ids'].to(device)  # Move the input_ids to the selected device\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Move the attention_mask to the selected device\n",
    "        labels = batch['labels'].to(device)  # Move the labels to the selected device\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # Forward pass: compute model output and loss\n",
    "        loss = outputs.loss  # Extract the loss from the model output\n",
    "        logits = outputs.logits  # Extract the logits (raw predictions) from the model output\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "        _, preds = torch.max(logits, dim=1)  # Get the predicted class with the highest score\n",
    "        correct_predictions += torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        total_samples += labels.size(0)  # Update the total number of samples\n",
    "\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        scheduler.step()  # Update the learning rate using the scheduler\n",
    "        optimizer.zero_grad()  # Clear the gradients for the next step\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)  # Calculate the average loss over the epoch\n",
    "    accuracy = correct_predictions.double() / total_samples  # Calculate the accuracy over the epoch\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')  # Print the loss and accuracy for the current epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score on the validation set: 0.9127\n",
      "\n",
      "Full Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CL       0.98      0.98      0.98      1866\n",
      "          CR       0.91      0.92      0.91      1835\n",
      "          DC       0.84      0.80      0.82      1355\n",
      "          DS       0.92      0.94      0.93      1774\n",
      "          LO       0.92      0.91      0.91      1217\n",
      "          NI       0.92      0.91      0.91      1826\n",
      "          SE       0.88      0.91      0.90      1327\n",
      "\n",
      "    accuracy                           0.91     11200\n",
      "   macro avg       0.91      0.91      0.91     11200\n",
      "weighted avg       0.91      0.91      0.91     11200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report  # Import classification_report to generate a detailed performance report\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()  # Disable dropout and other training-specific layers\n",
    "\n",
    "predictions = []  # List to store model predictions\n",
    "true_labels = []  # List to store true labels\n",
    "\n",
    "# No gradient calculation is needed during evaluation\n",
    "with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "    for batch in validation_loader:  # Iterate over the validation DataLoader\n",
    "        input_ids = batch['input_ids'].to(device)  # Move input_ids to the selected device (GPU)\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Move attention_mask to the selected device\n",
    "        labels = batch['labels'].to(device)  # Move labels to the selected device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Get model outputs\n",
    "        logits = outputs.logits  # Extract logits (raw predictions)\n",
    "        preds = torch.argmax(logits, dim=1)  # Get the index of the max logit to determine the predicted class\n",
    "\n",
    "        predictions.append(preds)  # Store predictions in GPU tensors\n",
    "        true_labels.append(labels)  # Store true labels in GPU tensors\n",
    "\n",
    "# Concatenate all predictions and true labels from all batches\n",
    "predictions = torch.cat(predictions)  # Concatenate all the predictions\n",
    "true_labels = torch.cat(true_labels)  # Concatenate all the true labels\n",
    "\n",
    "# Convert predictions and true labels to CPU-based NumPy arrays\n",
    "predictions = predictions.cpu().numpy()  # Move to CPU and convert to NumPy array\n",
    "true_labels = true_labels.cpu().numpy()  # Move to CPU and convert to NumPy array\n",
    "\n",
    "# Decode predictions and true labels back to their original class labels\n",
    "predictions_decoded = label_encoder.inverse_transform(predictions)  # Convert predicted integers back to original labels\n",
    "true_labels_decoded = label_encoder.inverse_transform(true_labels)  # Convert true label integers back to original labels\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels_decoded, predictions_decoded, output_dict=True)  # Generate a detailed classification report as a dictionary\n",
    "weighted_f1_score = report['weighted avg']['f1-score']  # Extract the weighted F1 score from the report\n",
    "\n",
    "# Print the weighted F1 score and the full classification report\n",
    "print(f\"Weighted F1 Score on the validation set: {weighted_f1_score:.4f}\")  # Print the weighted F1 score with 4 decimal places\n",
    "print(\"\\nFull Classification Report:\\n\")\n",
    "print(classification_report(true_labels_decoded, predictions_decoded))  # Print the full classification report in a readable format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaurav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
